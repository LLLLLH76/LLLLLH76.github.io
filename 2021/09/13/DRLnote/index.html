<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"lllllh76.github.io","root":"/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":-1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="Note for Deep Reinforcement Learning">
<meta property="og:type" content="article">
<meta property="og:title" content="DRL note">
<meta property="og:url" content="https://lllllh76.github.io/2021/09/13/DRLnote/index.html">
<meta property="og:site_name" content="LLH&#39;s blog">
<meta property="og:description" content="Note for Deep Reinforcement Learning">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://lllllh76.github.io/2021/09/13/DRLnote/1.png">
<meta property="article:published_time" content="2021-09-13T11:57:05.000Z">
<meta property="article:modified_time" content="2021-09-13T12:04:58.904Z">
<meta property="article:author" content="LLH">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://lllllh76.github.io/2021/09/13/DRLnote/1.png">

<link rel="canonical" href="https://lllllh76.github.io/2021/09/13/DRLnote/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>DRL note | LLH's blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">LLH's blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://lllllh76.github.io/2021/09/13/DRLnote/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="C://Users//xxx//Downloads//Wallpapers//superman.jpg">
      <meta itemprop="name" content="LLH">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LLH's blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          DRL note
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2021-09-13 19:57:05 / Modified: 20:04:58" itemprop="dateCreated datePublished" datetime="2021-09-13T19:57:05+08:00">2021-09-13</time>
            </span>

          
            <div class="post-description">Note for Deep Reinforcement Learning</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h2 id="3-4-价值函数"><a href="#3-4-价值函数" class="headerlink" title="3.4 价值函数"></a>3.4 价值函数</h2><p>状态价值函数 V_pi ( s_t ), 仅依赖于 s_t, 而 a_t, s_t+1, a_t+1,…都被期望消掉了</p>
<p>动作价值函数 $Q(s_t, a_t)$, 仅依赖于 $s_t, a_t,$ 而 $s_{t+1}, a_{t+1}$,…都被期望消掉了</p>
<p>最优动作价值函数 $Q_\star (s,a) = \max_\pi Q (s,a)$, 若 $Q_\star$ 已知, 给定当前状态 $s$, 策略为选择使 $Q_\star$ 最大的 $a$</p>
<p>eg. $Q_\star(s, up) = 370, Q_\star(s, left) = -120, Q_\star(s, right) = 30$, 选 up.</p>
<p>已知 $s_t, a_t$, 无论未来采取什么策略, 回报 $U_t = R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + …$ 的期望不可能超过 $Q_\star$</p>
<p>DQN 学习出 $Q_\star$</p>
<h1 id="4-DQN-与-Q学习"><a href="#4-DQN-与-Q学习" class="headerlink" title="4 DQN 与 Q学习"></a>4 DQN 与 Q学习</h1><p>$Q_\star(s_t, a_t) = \mathbb{E}<em>{s</em>{t+1} \sim p(\cdot | s_t, a_t)}\left[R_t + \gamma\max\limits_{a\in \mathcal{A}}Q_\star(s_{t+1}, a) \mid s_t, a_t\right]$</p>
<p>DQN 本质为对 $Q_\star$ 的函数近似</p>
<p>状态空间可以无限, 动作空间需要有限.</p>
<p>网络输入为状态 $s$, 输出每个动作的 $Q$ 值</p>
<p>收集样本 $(s_t, a_t, r_t, s_{t+1})$ 反向更新网络参数 $w$</p>
<p>DQN 属于异策略 (off-policy): 用于收集经验的<strong>行为策略</strong>($\epsilon$ greedy)与控制 Agent 的<strong>目标策略</strong>不同</p>
<p>SARSA 属于同策略 (on-policy)</p>
<h1 id="5-Sarsa"><a href="#5-Sarsa" class="headerlink" title="5 Sarsa"></a>5 Sarsa</h1><p>用于训练评价策略 $Q_\pi$</p>
<p>$Q_\pi(s_t, a_t) = \mathbb{E}<em>{s</em>{t+1},a_{t+1}}\left[R_t + \gamma Q_\pi(s_{t+1}, a_{t+1}) \mid s_t, a_t\right]$</p>
<h1 id="6-价值学习高级技巧"><a href="#6-价值学习高级技巧" class="headerlink" title="6 价值学习高级技巧"></a>6 价值学习高级技巧</h1><h3 id="经验回放"><a href="#经验回放" class="headerlink" title="经验回放"></a>经验回放</h3><p>存储多个四元组反复训练网络, 只适用于异策略, 否则对于同策略, 存储的都是过时策略的四元组</p>
<ul>
<li>优先经验回放: 数组样本的TD误差越大, 抽取到该样本用于训练网络的概率越大 (少见 危险情况)</li>
</ul>
<p>然而 DQN 的训练方法是自举的 (用一个估算去更新同类的估算), 这会导致偏差传播, 即高估会导致更高估</p>
<ul>
<li>见第4部分的第一条式子, 右边的max为现有估计, 若它被高估了, 则需要更新的等式左边也会被高估<ul>
<li>目标网络: 相当于新建一个 DQN, 用来算 TD 目标, 更新原 DQN, 再一定程度更新目标网络. 参数仍然与原 DQN 相关, 只能缓解自举, 不能根治.</li>
</ul>
</li>
<li>max本身就会使有噪声的无偏随机变量被高估<ul>
<li>双Q: 用 DQN 算 $a_\star$ , 用目标网络算 TD 目标.</li>
</ul>
</li>
</ul>
<p>对决网络: $Q(s,a;w) = V(s;w^V) + D(s,a;w^D) - \max\limits_{a \in \mathcal{A}}D(s,a;w^D)$. 训练最优优势 $D_\star$ 与最优状态价值 $V_\star$ 两个网络从而近似 $Q_\star$</p>
<p>噪声网络: 参数 $w$ 的每个参数 $w_i = \mu_i + \sigma_i \cdot \xi_i$, 即 均值+标准差x噪声. 每做一次决策, 噪声需重新随机生成. 参数数量多一倍. 可以不用 $\epsilon-$greedy</p>
<h1 id="7-策略梯度"><a href="#7-策略梯度" class="headerlink" title="7 策略梯度"></a>7 策略梯度</h1><p>$\max\limits_{\theta} J(\theta) = \mathbb{E}_s[V_\pi(s)]$</p>
<p>梯度上升</p>
<p>$\nabla_\theta J(\theta)$ 的一个无偏估计为 $g(s,a;\theta) = Q_\pi(s,a) \cdot \nabla_\theta \ln \pi(a\mid s;\theta)$</p>
<p>$\theta \leftarrow \theta + \beta\cdot g(s,a;\theta)$​</p>
<p>策略网络: 针对有限的动作空间, 输入状态 $s$, 输出各动作概率 $\pi(\cdot \mid s_t; \theta)$​</p>
<h3 id="Reinforce"><a href="#Reinforce" class="headerlink" title="Reinforce"></a>Reinforce</h3><p>玩一盘游戏得到 $u_t = r_t + \gamma r_{t+1} + …$ 用其近似 $Q_\pi(s,a)$</p>
<p>同策略 (策略网络)</p>
<h3 id="Actor-Critic"><a href="#Actor-Critic" class="headerlink" title="Actor-Critic"></a>Actor-Critic</h3><p>策略网络(Actor) + 价值网络(Critic)</p>
<p>训练价值网络近似 $Q_\pi(s,a)$</p>
<p>训练算法为 SARSA, 同策略</p>
<p>也可以用目标网络 (价值网络) 缓解自举</p>
<h1 id="8-带基线的策略梯度"><a href="#8-带基线的策略梯度" class="headerlink" title="8 带基线的策略梯度"></a>8 带基线的策略梯度</h1><h3 id="Reinforce-1"><a href="#Reinforce-1" class="headerlink" title="Reinforce"></a>Reinforce</h3><p>$g_b(s,a;\theta) = [ Q_\pi(s,a) - b ] \cdot \nabla_\theta \ln \pi(a\mid s;\theta)$ </p>
<p>只要 $b$ 不依赖与动作 $A$, $g_b(s,a;\theta)$ 依然是 $\nabla_\theta J(\theta)$ 的一个无偏估计</p>
<p>可令 $b = V_\pi(s)$, 新建价值网络近似 $V_\pi(s)$</p>
<p>Reinforce $u_t$​ 近似 $Q_\pi(s,a)$​, 同时基于 $u_t$​​ 更新价值网络的参数</p>
<hr>
<h3 id="Advantage-Actor-Critic-A2C"><a href="#Advantage-Actor-Critic-A2C" class="headerlink" title="Advantage Actor-Critic (A2C)"></a>Advantage Actor-Critic (A2C)</h3><p>恰好 $Q_\pi(s,a) - V_\pi(s)$​ 为优势函数​ Advantage Function</p>
<p>策略网络与价值网络的结构与带基线的 Reinforce 相同, 训练方法不同.</p>
<p>训练价值网络:</p>
<ul>
<li><p>基于 Bellman 方程 $V_\pi(s_t) = \mathbb{E}<em>{A_t,S_t} [R_t + \gamma \cdot V_\pi(S</em>{t+1})]$</p>
</li>
<li><p>TD 目标为 $r_t + \gamma \cdot V(s_{t+1};w)$​</p>
</li>
</ul>
<p>训练策略网络:</p>
<ul>
<li>$g(s_t,a_t;\theta) = [ r_t + \gamma \cdot V(s_{t+1};w) - V(s_{t};w) ] \cdot \nabla_\theta \ln \pi(a\mid s;\theta)$​</li>
</ul>
<p>同策略 (策略网络)</p>
<h1 id="9-策略学习高级技巧"><a href="#9-策略学习高级技巧" class="headerlink" title="9 策略学习高级技巧"></a>9 策略学习高级技巧</h1><h3 id="TRPO"><a href="#TRPO" class="headerlink" title="TRPO"></a>TRPO</h3><p>Trust Region Policy Optimization 置信域策略优化</p>
<p>置信域 $\mathcal{N}(\theta_{now})$: $\theta_{now}$​ 的一个邻域, 在这个邻域内的 $\theta$​, 函数 $L(\theta \mid \theta_{now})$ 很接近 $J(\theta)$​.<br>$J(\theta) = \mathbb{E}_S [V_\pi(S)]$</p>
<p>$ = \mathbb{E}<em>S [\sum\limits</em>{a\in \mathcal{A}} \pi(a\mid S;\theta)Q_\pi(S,a)]$​</p>
<p>$= \mathbb{E}<em>S [\sum\limits</em>{a\in \mathcal{A}} \pi(a\mid S;\theta_{now})\frac{\pi(a\mid s;\theta)}{\pi(a\mid S;\theta_{now})}Q_\pi(S,a)]$​​</p>
<p>$= \mathbb{E}<em>S [\mathbb{E}</em>{A \sim \pi(\cdot \mid S; \theta_{now})} [\frac{\pi(A\mid S;\theta)}{\pi(A\mid s;\theta_{now})}Q_\pi(S,A)]]$​</p>
<ol>
<li><p>做近似<br> 由轨迹蒙特卡洛<br> $L(\theta \mid \theta_{now}) = \frac{1}{n} \sum\limits_{t=1}^n \frac{\pi(a_t\mid s_t;\theta)}{\pi(a_t\mid s_t;\theta_{now})} \cdot u_t$</p>
</li>
<li><p>最大化</p>
</li>
</ol>
<p>  $\max_\theta L(\theta \mid \theta_{now}); \textsf{st} ~\theta \in \mathcal{N}(\theta_{now})$</p>
<p>  用球或 KL 散度($\pi$​ 的接近程度) 决定邻域</p>
<h3 id="熵正则-Entropy-Regularization"><a href="#熵正则-Entropy-Regularization" class="headerlink" title="熵正则 Entropy Regularization"></a>熵正则 Entropy Regularization</h3><p>对概率分布 $p = [p_1,…,p_n]$, Entropy$(p) = -\sum\limits_{i=1}^n p_i\ln p_i$</p>
<p>增大熵以增强策略的随机性(探索能力), 作为正则项加入目标函数</p>
<p>定义 $H(s;\theta) = \textsf{Entropy}(\pi(\cdot\mid s;\theta)) = -\sum\limits_{a \in \mathcal{A}} \pi(a \mid s;\theta)\ln \pi(a\mid s;\theta)$</p>
<p>$\max_{\theta} ~j(\theta) + \lambda\mathbb{E}_S[H(S;\theta)]$</p>
<p>策略梯度 (Reinforce, AC), TRPO等求解</p>
<h1 id="10-连续控制"><a href="#10-连续控制" class="headerlink" title="10 连续控制"></a>10 连续控制</h1><p>动作空间为连续集合 [0,1] vs {0,1}</p>
<h3 id="转化为离散-网格化"><a href="#转化为离散-网格化" class="headerlink" title="转化为离散 (网格化)"></a>转化为离散 (网格化)</h3><p>会维度灾难</p>
<h3 id="确定策略梯度-DPG"><a href="#确定策略梯度-DPG" class="headerlink" title="确定策略梯度 DPG"></a>确定策略梯度 DPG</h3><p>Deterministic Policy Gradient</p>
<p>Actor-Critic 其中策略网络的输出由各动作的概率向量变为一个确定的动作 $a = \mu(s;\theta)$</p>
<p>异策略 行为策略为旧策略网络+噪声</p>
<p>价值网络打分 $q(s, \mu(s;\theta); w)$</p>
<p>$\max_{\theta} J(\theta) = \mathbb{E}[q(s, \mu(s;\theta); w)]$</p>
<p>$g_j = \nabla_{\theta} q(s, \mu(s;\theta);w) = \nabla_{\theta} \mu(s;\theta)\cdot\nabla_{\mu(s;\theta)}(q(s, \mu(s;\theta);w))$</p>
<p>梯度上升训练策略网络</p>
<p>TD 算法训练价值网络</p>
<h5 id="双延时确定策略梯度-TD3"><a href="#双延时确定策略梯度-TD3" class="headerlink" title="双延时确定策略梯度 TD3"></a>双延时确定策略梯度 TD3</h5><p>缓解最大化与自举问题                                                                                                                                                                                                                                                                                                                                                                                                                                             </p>
<p>Twin Delayed Deep Deterministic Policy Gradient</p>
<ul>
<li>1 策略网络</li>
<li>1 目标策略网络</li>
<li>2 价值网络</li>
<li>2 目标价值网络</li>
</ul>
<p>取两个目标价值网络的较低分数作为 TD 目标 (截断双 Q 学习)</p>
<p>还可往动作中加噪声, 减小除价值网络外的网络更新频率 (先让价值网络慢慢学好一点)</p>
<h3 id="随机高斯策略"><a href="#随机高斯策略" class="headerlink" title="随机高斯策略"></a>随机高斯策略</h3><p>从高斯分布获得动作, 均值 $\mu(s)$​, 标准差 $\sigma(s)$, 即每个状态对应一个高斯分布.</p>
<p>$\pi(a \mid s ) = \frac{1}{\sqrt{2\pi}\sigma(s)} \cdot \exp(-\frac{[a - \mu(s)]^2}{2\sigma^2(s)})$​​​ (动作为一维)</p>
<p>构造均值网络 $\mu(s;\theta)$​ 与方差对数 ($\ln \sigma^2$​) 网络 $\rho (s;\theta)$$\pi(a \mid s ; \theta) = \Pi_{i=1}^d \frac{1}{\sqrt{2\pi} \exp[\rho_i(s;\theta)]} \cdot \exp(-\frac{[a_i - \mu_i(s;\theta)]^2}{2\exp[\rho_i(s;\theta)]})$​</p>
<p>实际中定义辅助网络 $f(s,a;\theta) = -\frac{1}{2}\sum\limits_{i=1}^d(\rho_i(s;\theta) + \frac{[a_i - \mu_i(s;\theta)]^2}{\exp[\rho_i(s;\theta)]})$</p>
<p>$f(s,a;\theta) = \ln\pi(a\mid s; \theta) + \textsf{constant}$</p>
<p>策略梯度 $g = Q_\pi(s,a) \nabla_{\theta} \ln\pi(a \mid s;\theta) = Q_\pi(s,a) \nabla_{\theta} f(s,a;\theta)$</p>
<p>用 Reinforce with baseline, A2C 近似 $Q_\pi(s,a)$</p>
<h1 id="十一-对状态的不完全观测"><a href="#十一-对状态的不完全观测" class="headerlink" title="十一 对状态的不完全观测"></a>十一 对状态的不完全观测</h1><p>$\pi(a \mid s ; \theta) \rightarrow \pi(a \mid o_{1:t} ; \theta)$</p>
<p>t 可变, 即网络的输入形状可变, 不能直接使用卷积层/全连接层.</p>
<p>循环神经网络 RNN Recurrent NN, 把一个序列映射到一个特征向量</p>
<p>$(x_1,…,x_i) \rightarrow h_i, h_i(i=1,…,n)$ 的维度都相同, 且 $h_i$ 包含 $x_1,…,x_i$ 的所有信息, 也即最后只需保留 $h_n$​, 作为传统输入送给神经网络.​</p>
<p>$h_t = \tanh (W[h_{t-1};x_t]+b)$, 矩阵 $W$, 向量 $b$​ 为 RNN 的参数.​</p>
<p>当 RNN 作为策略网络, 只需将 $h_t$ 作为全连接网络的输入, 输出 $|\mathcal{A}|$ 维的动作概率向量.</p>
<p>类似搭建 DQN $Q(o_{1:t}, a_t; w)$ 与价值网络 $q(o_{1:t}, a_t; w)$</p>
<h1 id="十二-并行计算"><a href="#十二-并行计算" class="headerlink" title="十二 并行计算"></a>十二 并行计算</h1><p>重复以下步骤直到收敛</p>
<ol>
<li>广播 (Broadcast): 服务器广播模型参数到所有节点 (老板给打工仔发任务)</li>
<li>映射 (Map): 节点用本地数据计算 (打工仔干活)</li>
<li>规约 (Reduce): 节点将信息发送回服务器 (打工仔交活儿)</li>
<li>更新参数: 老板准备明天干什么活儿?</li>
</ol>
<ul>
<li>同步算法: 所有节点都完成映射计算后, 系统才能执行规约通信 (短板效应)</li>
<li>异步算法: 节点之间不等待, 需要服务器能够与每个节点单独通信, 接收到任一节点的结果后就更新参数, 即不同节点的参数通常是不同的.</li>
</ul>
<p>异步算法可用于 DQN, Asynchronous A2C (A3C), 服务器处理网络参数.</p>
<h1 id="十三-多智能体系统"><a href="#十三-多智能体系统" class="headerlink" title="十三 多智能体系统"></a>十三 多智能体系统</h1><ul>
<li><p>合作关系 Fully Cooperative</p>
<p>$R_t^1 = R_t^2 = … = R_t^m, m\textsf{个Agent},~\forall t$​​</p>
</li>
<li><p>竞争关系 Fully Competitive</p>
<p>双方奖励负相关, 例如零和博弈, 奖励之和为 0</p>
</li>
<li><p>合作竞争混合 Mixed Cooperative &amp; Competitive</p>
<p>分为多个群组, 组内合作, 组件竞争, 例如踢足球</p>
</li>
<li><p>利己主义 Self-Interested</p>
<p>只想最大化自身累计奖励</p>
</li>
</ul>
<p>策略网络为 $\pi(\cdot \mid s; \theta^i)$​ or $\mu(s;\theta^i)$</p>
<p>动作价值函数 $Q_\pi^i(s_t,a_t) = \mathbb{E}<em>{S</em>{t+1},…,S_n,A_{t+1},…,A_n}[U_t^i \mid S_t=s_t, A_t = a_t]$</p>
<p>整体动作 $A$ 的概率密度函数 $\pi(A\mid S;\theta^1,…,\theta^m) = \Pi_{i=1}^m \pi(A^i \mid S;\theta^i)$</p>
<p>状态价值函数 $V_\pi^i(s) = \mathbb{E}<em>A[Q_\pi^i(s,A)] = \sum\limits</em>{a^1 \in \mathcal{A}^1}\sum\limits_{a^2 \in \mathcal{A}^2}…\sum\limits_{a^m \in \mathcal{A}^m} \pi(A\mid S;\theta^1,…,\theta^m)Q_\pi^i(s,a)$</p>
<h1 id="十四-合作关系-MARL"><a href="#十四-合作关系-MARL" class="headerlink" title="十四 合作关系 MARL"></a>十四 合作关系 MARL</h1><p>$S = [O^1,O^2,…,O^m]$</p>
<p>$R,U,Q,V,J$ 均相同</p>
<h3 id="MAC-A2C"><a href="#MAC-A2C" class="headerlink" title="MAC-A2C"></a>MAC-A2C</h3><p>Multi-Agent Cooperative Advantage Actor-Critic</p>
<ul>
<li><p>价值网络: $v(s;w)$​​. 所有 Agent 共用, 输入为上方定义的 s, 输出为对 s 的评分 .TD 算法训练.</p>
</li>
<li><p>策略网络: $\pi(\cdot \mid s;\theta^i)$​. 每个 Agent 独有, 输入为 s, 输出为 $\mathcal{A}^i$ 的概率分布</p>
<p>$\nabla_{\theta^i} J(\theta^1,…,\theta^m) = \mathbb{E}<em>{S,A}[(Q_\pi(S,A) - V_\pi(S))\cdot\nabla</em>{\theta^i} \ln\pi(A^i\mid S;\theta^i)]$</p>
<p>$g^i(s,a;\theta^i) = (Q_\pi(s,a) - V_\pi(s))\cdot\nabla_{\theta^i} \ln\pi(a^i\mid s;\theta^i)$</p>
<p>$g^i(s_t,a_t;\theta^i) = (r_t+\gamma \cdot v(s_{t+1};w) - v_\pi(s_t;w))\cdot\nabla_{\theta^i} \ln\pi(a_t^i\mid s_t;\theta^i)$</p>
</li>
</ul>
<p>同策略 $\pi(\cdot\mid s_t;\theta^i)$</p>
<p>Agent 不能独立做决策, 需要知道所有 Agent 的观测, 即全局状态.</p>
<p><img src="1.png"></p>
<p>trade-off: 通信慢, 但不近似网络, 效果好.</p>
<ul>
<li>中心化训练+中心化决策: 1 个价值网络 + m 个策略网络, 中央收集各 Agent 的观测, 分配决策, Agent 完全不用思考. 由于必须等待所有观测组成状态, 产生短板效应, 延迟高.</li>
<li>去中心化训练+去中心化决策: m 个价值网络(输入为 $o^i$ 而非 $s$) + m 个策略网络(输入为 $o^i$ 而非 $s$), Agent 之间不通信, 参数独立, 本质为 SARL.</li>
<li>中心化训练+去中心化决策: 1 个价值网络 + m 个策略网络(输入为 $o^i$ 而非 $s$), 中央收集各 Agent 的观测, 训练价值网络的同时将 TD 误差广播到所有 Agent ,每个 Agent 更新自己的策略网络.</li>
</ul>
<h1 id="十五-非合作关系-MARL"><a href="#十五-非合作关系-MARL" class="headerlink" title="十五 非合作关系 MARL"></a>十五 非合作关系 MARL</h1><p>$S = [O^1,O^2,…,O^m]$</p>
<p>$R,U,Q,V,J$​​ 不相同</p>
<p>$J^i(\theta^1,…,\theta^m) = \mathbb{E}[V_\pi^i(s)]$​</p>
<p>每个 Agent 求解 $\max_{\theta^i} J^i(\theta^1,…,\theta^m)$</p>
<p>收敛判别为纳什均衡: 当其余所有 Agent 都不改变策略的情况下, 一个 Agent $i$ 单独改变策略 $\theta^i$​ 无法令 $J^i$ 变大.</p>
<p>两次学习优劣评价: 交换对手.</p>
<h3 id="MAN-A2C"><a href="#MAN-A2C" class="headerlink" title="MAN-A2C"></a>MAN-A2C</h3><p>Multi-Agent Non-cooperative A2C</p>
<p>m 个策略网络 + m 个价值网络</p>
<p>$\nabla_{\theta^i} J(\theta^1,…,\theta^m) = \mathbb{E}<em>{S,A}[(Q_\pi^i(S,A) - V_\pi^i(S))\cdot\nabla</em>{\theta^i} \ln\pi(A^i\mid S;\theta^i)]$​</p>
<p>  $g^i(s_t,a_t^i;\theta^i) = (r_t^i+\gamma \cdot v(s_{t+1};w^i) - v_\pi(s_t;w^i))\cdot\nabla_{\theta^i} \ln\pi(a_t^i\mid s_t;\theta^i)$​</p>
<p>同策略 $\pi(\cdot\mid s_t;\theta^i)$​</p>
<ul>
<li>中心化训练+中心化决策: 中央收集各 Agent 的观测, m 个评委分别评价各个Agent, m 个策略网络产生动作.</li>
<li>去中心化训练+去中心化决策: 价值网络与策略网络输入为 $o^i$ 而非 $s$, Agent 之间不通信, 参数独立, 奖赏各不相同. 本质为 SARL.</li>
<li>中心化训练+去中心化决策: 策略网络输入为 $o^i$ 而非 $s$​​​​, 与 MAC-A2C 的中心化训练+去中心化决策基本相同, 区别在于有 m 个价值网络而非 1 个.</li>
</ul>
<hr>
<h3 id="MADDPG"><a href="#MADDPG" class="headerlink" title="MADDPG"></a>MADDPG</h3><p>Multi-Agent Deep Deterministic Policy Gradient 多智能体深度确定策略梯度</p>
<p>连续控制 异策略 中心化训练+去中心化决策</p>
<p>训练单个 Agent 的策略网络与价值网络需要一个经验回放与所有策略网络, 因此只能中心化训练.</p>
<h1 id="十六-注意力机制与-MARL"><a href="#十六-注意力机制与-MARL" class="headerlink" title="十六 注意力机制与 MARL"></a>十六 注意力机制与 MARL</h1><p>自注意力层 Self-Attention Layer: 输入为 $(x^1,…,x^m)$, 输出为 $(c^1,…,c^m)$, $m$ 可变, $c^i$ 与 $(x^1,…,x^m)$ 相关. 此前的 RNN 只与  $(x^1,…,x^i)$​ 相关.</p>
<p>$ q^i = W_q x^i ,  ~k^i = W_k x^i , ~v^i = W_v x^i$</p>
<p>$\alpha^i = softmax[(q^i)^\top k^1,…,(q^i)^\top k^m]$</p>
<p>$c^i = [v^1,…,v^m]\cdot \alpha^i = \alpha_1^i v^1 + … + \alpha_m^i v^m$</p>
<p>$c^i$ 主要取决于最密切相关的一个或几个 $x<del>\rightarrow</del> $一个 Agent 学会判断哪些 Agent 最相关并重点关注. $c^i$ 作为全连接网络的输入. </p>
<p>多头(Multi-Head)自注意力层将单头(Single-Head)自注意力层的输出连接起来.</p>
<p>可用于策略网络与价值网络, 离散与连续.</p>
<p>可将中心化的 $m$ 个价值网络/策略网络用 1 个带自注意力层的网络实现. $m$​ 越大效果越好.</p>
<h1 id="十七-模仿学习"><a href="#十七-模仿学习" class="headerlink" title="十七 模仿学习"></a>十七 模仿学习</h1><p>Imitation Learning</p>
<p>不是强化学习. 通过向人类专家学习策略网络, 使其做出与专家相同决策, 而非强化学习使累计奖励最大化. 训练成本低 (无人驾驶不用撞车).</p>
<h3 id="行为克隆"><a href="#行为克隆" class="headerlink" title="行为克隆"></a>行为克隆</h3><p>本质为监督学习, 数据集为 {(状态, 动作)}, 不需要与环境交互.</p>
<ul>
<li>连续控制: 回归</li>
<li>离散控制: 多类别分类器, 输出各动作概率</li>
</ul>
<h3 id="逆向强化学习-Inverse-RL"><a href="#逆向强化学习-Inverse-RL" class="headerlink" title="逆向强化学习 Inverse RL"></a>逆向强化学习 Inverse RL</h3><p>根据人类专家的策略学习奖赏关于状态与动作的函数, 再用其训练策略.</p>
<h3 id="GAIL"><a href="#GAIL" class="headerlink" title="GAIL"></a>GAIL</h3><p>Generative Adversarial Imitation Learning</p>
<h4 id="生成判别网络-GAN"><a href="#生成判别网络-GAN" class="headerlink" title="生成判别网络 GAN"></a>生成判别网络 GAN</h4><p>Generative Adversarial Network</p>
<ul>
<li>生成器: $a = G(s;\theta)$, 由随机向量 $s$ 生成假样本</li>
<li>判别器: $\hat{p} = D(x;\Phi)$, 判断样本 $x$ 是真的概率</li>
</ul>
<p>同时训练两者, 增强生成器以假乱真与判别器判别的能力.</p>
<p>GAIL 的生成器是策略网络, 输入状态输出动作. 判别器输入状态, 输出 $|\mathcal{A}|$ 维向量 $\hat{p}$, 每个元素 $\hat{p}_a \in (0,1)$ 表示 $(s,a)$ 真假程度 ($a$ 是否为人类专家(真样本)所做)</p>
<h1 id="十八-AlphaGo-与-MCTS"><a href="#十八-AlphaGo-与-MCTS" class="headerlink" title="十八 AlphaGo 与 MCTS"></a>十八 AlphaGo 与 MCTS</h1>
    </div>

    
    
    

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2021/09/13/DSP_note/" rel="prev" title="DSP note">
      <i class="fa fa-chevron-left"></i> DSP note
    </a></div>
      <div class="post-nav-item">
    <a href="/2021/09/13/control_theory_note/" rel="next" title="Control Theory note">
      Control Theory note <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#3-4-%E4%BB%B7%E5%80%BC%E5%87%BD%E6%95%B0"><span class="nav-number">1.</span> <span class="nav-text">3.4 价值函数</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#4-DQN-%E4%B8%8E-Q%E5%AD%A6%E4%B9%A0"><span class="nav-number"></span> <span class="nav-text">4 DQN 与 Q学习</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#5-Sarsa"><span class="nav-number"></span> <span class="nav-text">5 Sarsa</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#6-%E4%BB%B7%E5%80%BC%E5%AD%A6%E4%B9%A0%E9%AB%98%E7%BA%A7%E6%8A%80%E5%B7%A7"><span class="nav-number"></span> <span class="nav-text">6 价值学习高级技巧</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BB%8F%E9%AA%8C%E5%9B%9E%E6%94%BE"><span class="nav-number">0.1.</span> <span class="nav-text">经验回放</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#7-%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6"><span class="nav-number"></span> <span class="nav-text">7 策略梯度</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Reinforce"><span class="nav-number">0.1.</span> <span class="nav-text">Reinforce</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Actor-Critic"><span class="nav-number">0.2.</span> <span class="nav-text">Actor-Critic</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#8-%E5%B8%A6%E5%9F%BA%E7%BA%BF%E7%9A%84%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6"><span class="nav-number"></span> <span class="nav-text">8 带基线的策略梯度</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Reinforce-1"><span class="nav-number">0.1.</span> <span class="nav-text">Reinforce</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Advantage-Actor-Critic-A2C"><span class="nav-number">0.2.</span> <span class="nav-text">Advantage Actor-Critic (A2C)</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#9-%E7%AD%96%E7%95%A5%E5%AD%A6%E4%B9%A0%E9%AB%98%E7%BA%A7%E6%8A%80%E5%B7%A7"><span class="nav-number"></span> <span class="nav-text">9 策略学习高级技巧</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#TRPO"><span class="nav-number">0.1.</span> <span class="nav-text">TRPO</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%86%B5%E6%AD%A3%E5%88%99-Entropy-Regularization"><span class="nav-number">0.2.</span> <span class="nav-text">熵正则 Entropy Regularization</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#10-%E8%BF%9E%E7%BB%AD%E6%8E%A7%E5%88%B6"><span class="nav-number"></span> <span class="nav-text">10 连续控制</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%BD%AC%E5%8C%96%E4%B8%BA%E7%A6%BB%E6%95%A3-%E7%BD%91%E6%A0%BC%E5%8C%96"><span class="nav-number">0.1.</span> <span class="nav-text">转化为离散 (网格化)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%A1%AE%E5%AE%9A%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6-DPG"><span class="nav-number">0.2.</span> <span class="nav-text">确定策略梯度 DPG</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%8F%8C%E5%BB%B6%E6%97%B6%E7%A1%AE%E5%AE%9A%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6-TD3"><span class="nav-number">0.2.0.1.</span> <span class="nav-text">双延时确定策略梯度 TD3</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%9A%8F%E6%9C%BA%E9%AB%98%E6%96%AF%E7%AD%96%E7%95%A5"><span class="nav-number">0.3.</span> <span class="nav-text">随机高斯策略</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%8D%81%E4%B8%80-%E5%AF%B9%E7%8A%B6%E6%80%81%E7%9A%84%E4%B8%8D%E5%AE%8C%E5%85%A8%E8%A7%82%E6%B5%8B"><span class="nav-number"></span> <span class="nav-text">十一 对状态的不完全观测</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%8D%81%E4%BA%8C-%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97"><span class="nav-number"></span> <span class="nav-text">十二 并行计算</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%8D%81%E4%B8%89-%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E7%B3%BB%E7%BB%9F"><span class="nav-number"></span> <span class="nav-text">十三 多智能体系统</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%8D%81%E5%9B%9B-%E5%90%88%E4%BD%9C%E5%85%B3%E7%B3%BB-MARL"><span class="nav-number"></span> <span class="nav-text">十四 合作关系 MARL</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#MAC-A2C"><span class="nav-number">0.1.</span> <span class="nav-text">MAC-A2C</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%8D%81%E4%BA%94-%E9%9D%9E%E5%90%88%E4%BD%9C%E5%85%B3%E7%B3%BB-MARL"><span class="nav-number"></span> <span class="nav-text">十五 非合作关系 MARL</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#MAN-A2C"><span class="nav-number">0.1.</span> <span class="nav-text">MAN-A2C</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#MADDPG"><span class="nav-number">0.2.</span> <span class="nav-text">MADDPG</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%8D%81%E5%85%AD-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E4%B8%8E-MARL"><span class="nav-number"></span> <span class="nav-text">十六 注意力机制与 MARL</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%8D%81%E4%B8%83-%E6%A8%A1%E4%BB%BF%E5%AD%A6%E4%B9%A0"><span class="nav-number"></span> <span class="nav-text">十七 模仿学习</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%A1%8C%E4%B8%BA%E5%85%8B%E9%9A%86"><span class="nav-number">0.1.</span> <span class="nav-text">行为克隆</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%80%86%E5%90%91%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-Inverse-RL"><span class="nav-number">0.2.</span> <span class="nav-text">逆向强化学习 Inverse RL</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#GAIL"><span class="nav-number">0.3.</span> <span class="nav-text">GAIL</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%94%9F%E6%88%90%E5%88%A4%E5%88%AB%E7%BD%91%E7%BB%9C-GAN"><span class="nav-number">0.3.1.</span> <span class="nav-text">生成判别网络 GAN</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%8D%81%E5%85%AB-AlphaGo-%E4%B8%8E-MCTS"><span class="nav-number"></span> <span class="nav-text">十八 AlphaGo 与 MCTS</span></a></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="LLH"
      src="C://Users//xxx//Downloads//Wallpapers//superman.jpg">
  <p class="site-author-name" itemprop="name">LLH</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">5</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">LLH</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://mist.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a>
  </div>

<div class="powered-by">
    <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <i class="fa fa-user-md"></i>
    <span id="busuanzi_container_site_uv">
        Vistors<span id="busuanzi_value_site_uv"></span>
    </span>
    <span class="post-meta-divider">|</span>
    <span id="busuanzi_container_site_pv">
        Hits<span id="busuanzi_value_site_pv"></span>
    </span>
</div>
        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  

</body>
</html>
