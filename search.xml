<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Control Theory note</title>
    <url>/2021/09/13/Control_Theory_note/</url>
    <content><![CDATA[<p>Control Theory</p>
<h1 id="Lecture-1"><a href="#Lecture-1" class="headerlink" title="Lecture 1"></a>Lecture 1</h1><p>控制: 使受控对象产生预期相应 (按照预期方式工作).</p>
<p>输入(激励) —&gt; 输出(相应)</p>
<p>自动控制: 无人参与情况下, 利用控制装置使受控对象的某一受控变量自动按照预定规律运行.</p>
<p>受控对象: 要求实现自动工作的机器, 设备或生产过程. 有多个受控变量.</p>
<ul>
<li><p>开环控制: 利用执行机构直接控制受控对象, 信息单向流动</p>
<ul>
<li>控制量 $u(t)$ —&gt; 执行机构(油门, 方向盘) —&gt; 受控对象 —&gt; 输出 $y(t)$​</li>
</ul>
</li>
<li><p>闭环控制: 有反馈过程, 信息双向流动</p>
<ul>
<li><p>预期输入 $r(t)$ —&gt; 比较器 —&gt; 误差$e(t)$ —&gt; 控制器 —&gt; 控制量$u(t)$ —&gt; 受控对象 —&gt; 实际输出 $y(t)$</p>
<p>​                                    $\uparrow$​​ &lt;———- 测量值 &lt;———- 测量装置 &lt;———- 被测变量 &lt;———- $\downarrow$​​</p>
</li>
</ul>
<h2 id="HW-1"><a href="#HW-1" class="headerlink" title="HW 1"></a>HW 1</h2></li>
</ul>
<h3 id="1-1"><a href="#1-1" class="headerlink" title="1-1"></a>1-1</h3><ul>
<li>输出变量: 激光器的实际输出功率</li>
<li>输入变量: 激光器预期输出的功率</li>
<li>待测变量: 传感器测量得到的, 与激光器的实际输出功率成比例的信号</li>
<li>控制装置: 微处理器</li>
</ul>
<h3 id="1-2"><a href="#1-2" class="headerlink" title="1-2"></a>1-2</h3><p>…</p>
<h1 id="Lecture-2"><a href="#Lecture-2" class="headerlink" title="Lecture 2"></a>Lecture 2</h1><ul>
<li>经典控制理论: 传递函数</li>
<li>现代控制理论: 状态空间方程</li>
</ul>
<p>都要微分方程</p>
<ul>
<li>时间域: 微分/差分方程, 状态方程</li>
<li>复数域: 传递函数, 结构图</li>
<li>频率域: 频率特性函数</li>
</ul>
<p>微分方程模型:</p>
<ul>
<li>确定输入量, 输出量</li>
<li>根据设定写方程</li>
<li>消除中间量, 保留输入, 输出</li>
<li>标准化: 等式右边/左边与输入/输出变量有关</li>
</ul>
<p>线性定常(时不变): 系数不变</p>
<p>线性系统:</p>
<ul>
<li>叠加性: $ y(x_1+x_2) = y(x_1) + y(x_2)$</li>
<li>齐次性: $y(\beta x ) = \beta y(x)$</li>
</ul>
<p>一般$n$​阶线性定常微分方程模型: $y(t)$ 为输出, $r(t)$ 为输入, $m&lt;n$, $a,b$ 均为实数, 为系统参数</p>
<p>$a<em>0 \frac{d^n}{dt^n}y(t) + a_1 \frac{d^{n-1}}{dt^{n-1}}y(t) + \cdots + a</em>{n-1} \frac{d^{}}{dt^{}}y(t) + a<em>n y_t = b_0 \frac{d^m}{dt^m}r(t) + b_1 \frac{d^{m-1}}{dt^{m-1}}r(t) + \cdots + b</em>{m-1} \frac{d^{}}{dt^{}}r(t) + b_m r_t $​</p>
<p>建立线性化近似模型策略:</p>
<ol>
<li>限制范围, 使系统工作在线性区, 忽略非线性. $y = \frac{dy}{dx}|_{x_0} x$​</li>
<li>小信号 : 泰勒展开</li>
</ol>
]]></content>
  </entry>
  <entry>
    <title>DRL note</title>
    <url>/2021/09/13/DRLnote/</url>
    <content><![CDATA[<h2 id="3-4-价值函数"><a href="#3-4-价值函数" class="headerlink" title="3.4 价值函数"></a>3.4 价值函数</h2><p>状态价值函数 V_pi ( s_t ), 仅依赖于 s_t, 而 a_t, s_t+1, a_t+1,…都被期望消掉了</p>
<p>动作价值函数 $Q(s<em>t, a_t)$, 仅依赖于 $s_t, a_t,$ 而 $s</em>{t+1}, a_{t+1}$,…都被期望消掉了</p>
<p>最优动作价值函数 $Q<em>\star (s,a) = \max</em>\pi Q (s,a)$, 若 $Q_\star$ 已知, 给定当前状态 $s$, 策略为选择使 $Q_\star$ 最大的 $a$</p>
<p>eg. $Q<em>\star(s, up) = 370, Q</em>\star(s, left) = -120, Q_\star(s, right) = 30$, 选 up.</p>
<p>已知 $s<em>t, a_t$, 无论未来采取什么策略, 回报 $U_t = R_t + \gamma R</em>{t+1} + \gamma^2 R<em>{t+2} + …$ 的期望不可能超过 $Q</em>\star$</p>
<p>DQN 学习出 $Q_\star$</p>
<h1 id="4-DQN-与-Q学习"><a href="#4-DQN-与-Q学习" class="headerlink" title="4 DQN 与 Q学习"></a>4 DQN 与 Q学习</h1><p>$Q<em>\star(s_t, a_t) = \mathbb{E}</em>{s<em>{t+1} \sim p(\cdot | s_t, a_t)}\left[R_t + \gamma\max\limits</em>{a\in \mathcal{A}}Q<em>\star(s</em>{t+1}, a) \mid s_t, a_t\right]$</p>
<p>DQN 本质为对 $Q_\star$ 的函数近似</p>
<p>状态空间可以无限, 动作空间需要有限.</p>
<p>网络输入为状态 $s$, 输出每个动作的 $Q$ 值</p>
<p>收集样本 $(s<em>t, a_t, r_t, s</em>{t+1})$ 反向更新网络参数 $w$</p>
<p>DQN 属于异策略 (off-policy): 用于收集经验的<strong>行为策略</strong>($\epsilon$ greedy)与控制 Agent 的<strong>目标策略</strong>不同</p>
<p>SARSA 属于同策略 (on-policy)</p>
<h1 id="5-Sarsa"><a href="#5-Sarsa" class="headerlink" title="5 Sarsa"></a>5 Sarsa</h1><p>用于训练评价策略 $Q_\pi$</p>
<p>$Q<em>\pi(s_t, a_t) = \mathbb{E}</em>{s<em>{t+1},a</em>{t+1}}\left[R<em>t + \gamma Q</em>\pi(s<em>{t+1}, a</em>{t+1}) \mid s_t, a_t\right]$</p>
<h1 id="6-价值学习高级技巧"><a href="#6-价值学习高级技巧" class="headerlink" title="6 价值学习高级技巧"></a>6 价值学习高级技巧</h1><h3 id="经验回放"><a href="#经验回放" class="headerlink" title="经验回放"></a>经验回放</h3><p>存储多个四元组反复训练网络, 只适用于异策略, 否则对于同策略, 存储的都是过时策略的四元组</p>
<ul>
<li>优先经验回放: 数组样本的TD误差越大, 抽取到该样本用于训练网络的概率越大 (少见 危险情况)</li>
</ul>
<p>然而 DQN 的训练方法是自举的 (用一个估算去更新同类的估算), 这会导致偏差传播, 即高估会导致更高估</p>
<ul>
<li>见第4部分的第一条式子, 右边的max为现有估计, 若它被高估了, 则需要更新的等式左边也会被高估<ul>
<li>目标网络: 相当于新建一个 DQN, 用来算 TD 目标, 更新原 DQN, 再一定程度更新目标网络. 参数仍然与原 DQN 相关, 只能缓解自举, 不能根治.</li>
</ul>
</li>
<li>max本身就会使有噪声的无偏随机变量被高估<ul>
<li>双Q: 用 DQN 算 $a_\star$ , 用目标网络算 TD 目标.</li>
</ul>
</li>
</ul>
<p>对决网络: $Q(s,a;w) = V(s;w^V) + D(s,a;w^D) - \max\limits<em>{a \in \mathcal{A}}D(s,a;w^D)$. 训练最优优势 $D</em>\star$ 与最优状态价值 $V<em>\star$ 两个网络从而近似 $Q</em>\star$</p>
<p>噪声网络: 参数 $w$ 的每个参数 $w_i = \mu_i + \sigma_i \cdot \xi_i$, 即 均值+标准差x噪声. 每做一次决策, 噪声需重新随机生成. 参数数量多一倍. 可以不用 $\epsilon-$greedy</p>
<h1 id="7-策略梯度"><a href="#7-策略梯度" class="headerlink" title="7 策略梯度"></a>7 策略梯度</h1><p>$\max\limits<em>{\theta} J(\theta) = \mathbb{E}_s[V</em>\pi(s)]$</p>
<p>梯度上升</p>
<p>$\nabla<em>\theta J(\theta)$ 的一个无偏估计为 $g(s,a;\theta) = Q</em>\pi(s,a) \cdot \nabla_\theta \ln \pi(a\mid s;\theta)$</p>
<p>$\theta \leftarrow \theta + \beta\cdot g(s,a;\theta)$​</p>
<p>策略网络: 针对有限的动作空间, 输入状态 $s$, 输出各动作概率 $\pi(\cdot \mid s_t; \theta)$​</p>
<h3 id="Reinforce"><a href="#Reinforce" class="headerlink" title="Reinforce"></a>Reinforce</h3><p>玩一盘游戏得到 $u<em>t = r_t + \gamma r</em>{t+1} + …$ 用其近似 $Q_\pi(s,a)$</p>
<p>同策略 (策略网络)</p>
<h3 id="Actor-Critic"><a href="#Actor-Critic" class="headerlink" title="Actor-Critic"></a>Actor-Critic</h3><p>策略网络(Actor) + 价值网络(Critic)</p>
<p>训练价值网络近似 $Q_\pi(s,a)$</p>
<p>训练算法为 SARSA, 同策略</p>
<p>也可以用目标网络 (价值网络) 缓解自举</p>
<h1 id="8-带基线的策略梯度"><a href="#8-带基线的策略梯度" class="headerlink" title="8 带基线的策略梯度"></a>8 带基线的策略梯度</h1><h3 id="Reinforce-1"><a href="#Reinforce-1" class="headerlink" title="Reinforce"></a>Reinforce</h3><p>$g<em>b(s,a;\theta) = [ Q</em>\pi(s,a) - b ] \cdot \nabla_\theta \ln \pi(a\mid s;\theta)$ </p>
<p>只要 $b$ 不依赖与动作 $A$, $g<em>b(s,a;\theta)$ 依然是 $\nabla</em>\theta J(\theta)$ 的一个无偏估计</p>
<p>可令 $b = V<em>\pi(s)$, 新建价值网络近似 $V</em>\pi(s)$</p>
<p>Reinforce $u<em>t$​ 近似 $Q</em>\pi(s,a)$​, 同时基于 $u_t$​​ 更新价值网络的参数</p>
<hr>
<h3 id="Advantage-Actor-Critic-A2C"><a href="#Advantage-Actor-Critic-A2C" class="headerlink" title="Advantage Actor-Critic (A2C)"></a>Advantage Actor-Critic (A2C)</h3><p>恰好 $Q<em>\pi(s,a) - V</em>\pi(s)$​ 为优势函数​ Advantage Function</p>
<p>策略网络与价值网络的结构与带基线的 Reinforce 相同, 训练方法不同.</p>
<p>训练价值网络:</p>
<ul>
<li><p>基于 Bellman 方程 $V<em>\pi(s_t) = \mathbb{E}</em>{A<em>t,S_t} [R_t + \gamma \cdot V</em>\pi(S_{t+1})]$</p>
</li>
<li><p>TD 目标为 $r<em>t + \gamma \cdot V(s</em>{t+1};w)$​</p>
</li>
</ul>
<p>训练策略网络:</p>
<ul>
<li>$g(s<em>t,a_t;\theta) = [ r_t + \gamma \cdot V(s</em>{t+1};w) - V(s<em>{t};w) ] \cdot \nabla</em>\theta \ln \pi(a\mid s;\theta)$​</li>
</ul>
<p>同策略 (策略网络)</p>
<h1 id="9-策略学习高级技巧"><a href="#9-策略学习高级技巧" class="headerlink" title="9 策略学习高级技巧"></a>9 策略学习高级技巧</h1><h3 id="TRPO"><a href="#TRPO" class="headerlink" title="TRPO"></a>TRPO</h3><p>Trust Region Policy Optimization 置信域策略优化</p>
<p>置信域 $\mathcal{N}(\theta<em>{now})$: $\theta</em>{now}$​ 的一个邻域, 在这个邻域内的 $\theta$​, 函数 $L(\theta \mid \theta<em>{now})$ 很接近 $J(\theta)$​.<br>$J(\theta) = \mathbb{E}_S [V</em>\pi(S)]$</p>
<p>$ = \mathbb{E}<em>S [\sum\limits</em>{a\in \mathcal{A}} \pi(a\mid S;\theta)Q_\pi(S,a)]$​</p>
<p>$= \mathbb{E}<em>S [\sum\limits</em>{a\in \mathcal{A}} \pi(a\mid S;\theta<em>{now})\frac{\pi(a\mid s;\theta)}{\pi(a\mid S;\theta</em>{now})}Q_\pi(S,a)]$​​</p>
<p>$= \mathbb{E}<em>S [\mathbb{E}</em>{A \sim \pi(\cdot \mid S; \theta<em>{now})} [\frac{\pi(A\mid S;\theta)}{\pi(A\mid s;\theta</em>{now})}Q_\pi(S,A)]]$​</p>
<ol>
<li><p>做近似<br> 由轨迹蒙特卡洛<br> $L(\theta \mid \theta<em>{now}) = \frac{1}{n} \sum\limits</em>{t=1}^n \frac{\pi(a<em>t\mid s_t;\theta)}{\pi(a_t\mid s_t;\theta</em>{now})} \cdot u_t$</p>
</li>
<li><p>最大化</p>
<p>$\max<em>\theta L(\theta \mid \theta</em>{now}); \textsf{st} ~\theta \in \mathcal{N}(\theta_{now})$</p>
<p>用球或 KL 散度($\pi$​ 的接近程度) 决定邻域</p>
</li>
</ol>
<h3 id="熵正则-Entropy-Regularization"><a href="#熵正则-Entropy-Regularization" class="headerlink" title="熵正则 Entropy Regularization"></a>熵正则 Entropy Regularization</h3><p>对概率分布 $p = [p<em>1,…,p_n]$, Entropy$(p) = -\sum\limits</em>{i=1}^n p_i\ln p_i$</p>
<p>增大熵以增强策略的随机性(探索能力), 作为正则项加入目标函数</p>
<p>定义 $H(s;\theta) = \textsf{Entropy}(\pi(\cdot\mid s;\theta)) = -\sum\limits_{a \in \mathcal{A}} \pi(a \mid s;\theta)\ln \pi(a\mid s;\theta)$</p>
<p>$\max_{\theta} ~j(\theta) + \lambda\mathbb{E}_S[H(S;\theta)]$</p>
<p>策略梯度 (Reinforce, AC), TRPO等求解</p>
<h1 id="10-连续控制"><a href="#10-连续控制" class="headerlink" title="10 连续控制"></a>10 连续控制</h1><p>动作空间为连续集合 [0,1] vs {0,1}</p>
<h3 id="转化为离散-网格化"><a href="#转化为离散-网格化" class="headerlink" title="转化为离散 (网格化)"></a>转化为离散 (网格化)</h3><p>会维度灾难</p>
<h3 id="确定策略梯度-DPG"><a href="#确定策略梯度-DPG" class="headerlink" title="确定策略梯度 DPG"></a>确定策略梯度 DPG</h3><p>Deterministic Policy Gradient</p>
<p>Actor-Critic 其中策略网络的输出由各动作的概率向量变为一个确定的动作 $a = \mu(s;\theta)$</p>
<p>异策略 行为策略为旧策略网络+噪声</p>
<p>价值网络打分 $q(s, \mu(s;\theta); w)$</p>
<p>$\max_{\theta} J(\theta) = \mathbb{E}[q(s, \mu(s;\theta); w)]$</p>
<p>$g<em>j = \nabla</em>{\theta} q(s, \mu(s;\theta);w) = \nabla<em>{\theta} \mu(s;\theta)\cdot\nabla</em>{\mu(s;\theta)}(q(s, \mu(s;\theta);w))$</p>
<p>梯度上升训练策略网络</p>
<p>TD 算法训练价值网络</p>
<h5 id="双延时确定策略梯度-TD3"><a href="#双延时确定策略梯度-TD3" class="headerlink" title="双延时确定策略梯度 TD3"></a>双延时确定策略梯度 TD3</h5><p>缓解最大化与自举问题                                                                                                                                                                                                                                                                                                                                                                                                                                             </p>
<p>Twin Delayed Deep Deterministic Policy Gradient</p>
<ul>
<li>1 策略网络</li>
<li>1 目标策略网络</li>
<li>2 价值网络</li>
<li>2 目标价值网络</li>
</ul>
<p>取两个目标价值网络的较低分数作为 TD 目标 (截断双 Q 学习)</p>
<p>还可往动作中加噪声, 减小除价值网络外的网络更新频率 (先让价值网络慢慢学好一点)</p>
<h3 id="随机高斯策略"><a href="#随机高斯策略" class="headerlink" title="随机高斯策略"></a>随机高斯策略</h3><p>从高斯分布获得动作, 均值 $\mu(s)$​, 标准差 $\sigma(s)$, 即每个状态对应一个高斯分布.</p>
<p>$\pi(a \mid s ) = \frac{1}{\sqrt{2\pi}\sigma(s)} \cdot \exp(-\frac{[a - \mu(s)]^2}{2\sigma^2(s)})$​​​ (动作为一维)</p>
<p>构造均值网络 $\mu(s;\theta)$​ 与方差对数 ($\ln \sigma^2$​) 网络 $\rho (s;\theta)$$\pi(a \mid s ; \theta) = \Pi_{i=1}^d \frac{1}{\sqrt{2\pi} \exp[\rho_i(s;\theta)]} \cdot \exp(-\frac{[a_i - \mu_i(s;\theta)]^2}{2\exp[\rho_i(s;\theta)]})$​</p>
<p>实际中定义辅助网络 $f(s,a;\theta) = -\frac{1}{2}\sum\limits_{i=1}^d(\rho_i(s;\theta) + \frac{[a_i - \mu_i(s;\theta)]^2}{\exp[\rho_i(s;\theta)]})$</p>
<p>$f(s,a;\theta) = \ln\pi(a\mid s; \theta) + \textsf{constant}$</p>
<p>策略梯度 $g = Q<em>\pi(s,a) \nabla</em>{\theta} \ln\pi(a \mid s;\theta) = Q<em>\pi(s,a) \nabla</em>{\theta} f(s,a;\theta)$</p>
<p>用 Reinforce with baseline, A2C 近似 $Q_\pi(s,a)$</p>
<h1 id="十一-对状态的不完全观测"><a href="#十一-对状态的不完全观测" class="headerlink" title="十一 对状态的不完全观测"></a>十一 对状态的不完全观测</h1><p>$\pi(a \mid s ; \theta) \rightarrow \pi(a \mid o_{1:t} ; \theta)$</p>
<p>t 可变, 即网络的输入形状可变, 不能直接使用卷积层/全连接层.</p>
<p>循环神经网络 RNN Recurrent NN, 把一个序列映射到一个特征向量</p>
<p>$(x_1,…,x_i) \rightarrow h_i, h_i(i=1,…,n)$ 的维度都相同, 且 $h_i$ 包含 $x_1,…,x_i$ 的所有信息, 也即最后只需保留 $h_n$​, 作为传统输入送给神经网络.​</p>
<p>$h<em>t = \tanh (W[h</em>{t-1};x_t]+b)$, 矩阵 $W$, 向量 $b$​ 为 RNN 的参数.​</p>
<p>当 RNN 作为策略网络, 只需将 $h_t$ 作为全连接网络的输入, 输出 $|\mathcal{A}|$ 维的动作概率向量.</p>
<p>类似搭建 DQN $Q(o<em>{1:t}, a_t; w)$ 与价值网络 $q(o</em>{1:t}, a_t; w)$</p>
<h1 id="十二-并行计算"><a href="#十二-并行计算" class="headerlink" title="十二 并行计算"></a>十二 并行计算</h1><p>重复以下步骤直到收敛</p>
<ol>
<li>广播 (Broadcast): 服务器广播模型参数到所有节点 (老板给打工仔发任务)</li>
<li>映射 (Map): 节点用本地数据计算 (打工仔干活)</li>
<li>规约 (Reduce): 节点将信息发送回服务器 (打工仔交活儿)</li>
<li>更新参数: 老板准备明天干什么活儿?</li>
</ol>
<ul>
<li>同步算法: 所有节点都完成映射计算后, 系统才能执行规约通信 (短板效应)</li>
<li>异步算法: 节点之间不等待, 需要服务器能够与每个节点单独通信, 接收到任一节点的结果后就更新参数, 即不同节点的参数通常是不同的.</li>
</ul>
<p>异步算法可用于 DQN, Asynchronous A2C (A3C), 服务器处理网络参数.</p>
<h1 id="十三-多智能体系统"><a href="#十三-多智能体系统" class="headerlink" title="十三 多智能体系统"></a>十三 多智能体系统</h1><ul>
<li><p>合作关系 Fully Cooperative</p>
<p>$R_t^1 = R_t^2 = … = R_t^m, m\textsf{个Agent},~\forall t$​​</p>
</li>
<li><p>竞争关系 Fully Competitive</p>
<p>双方奖励负相关, 例如零和博弈, 奖励之和为 0</p>
</li>
<li><p>合作竞争混合 Mixed Cooperative &amp; Competitive</p>
<p>分为多个群组, 组内合作, 组件竞争, 例如踢足球</p>
</li>
<li><p>利己主义 Self-Interested</p>
<p>只想最大化自身累计奖励</p>
</li>
</ul>
<p>策略网络为 $\pi(\cdot \mid s; \theta^i)$​ or $\mu(s;\theta^i)$</p>
<p>动作价值函数 $Q<em>\pi^i(s_t,a_t) = \mathbb{E}</em>{S<em>{t+1},…,S_n,A</em>{t+1},…,A_n}[U_t^i \mid S_t=s_t, A_t = a_t]$</p>
<p>整体动作 $A$ 的概率密度函数 $\pi(A\mid S;\theta^1,…,\theta^m) = \Pi_{i=1}^m \pi(A^i \mid S;\theta^i)$</p>
<p>状态价值函数 $V<em>\pi^i(s) = \mathbb{E}_A[Q</em>\pi^i(s,A)] = \sum\limits<em>{a^1 \in \mathcal{A}^1}\sum\limits</em>{a^2 \in \mathcal{A}^2}…\sum\limits<em>{a^m \in \mathcal{A}^m} \pi(A\mid S;\theta^1,…,\theta^m)Q</em>\pi^i(s,a)$</p>
<h1 id="十四-合作关系-MARL"><a href="#十四-合作关系-MARL" class="headerlink" title="十四 合作关系 MARL"></a>十四 合作关系 MARL</h1><p>$S = [O^1,O^2,…,O^m]$</p>
<p>$R,U,Q,V,J$ 均相同</p>
<h3 id="MAC-A2C"><a href="#MAC-A2C" class="headerlink" title="MAC-A2C"></a>MAC-A2C</h3><p>Multi-Agent Cooperative Advantage Actor-Critic</p>
<ul>
<li><p>价值网络: $v(s;w)$​​. 所有 Agent 共用, 输入为上方定义的 s, 输出为对 s 的评分 .TD 算法训练.</p>
</li>
<li><p>策略网络: $\pi(\cdot \mid s;\theta^i)$​. 每个 Agent 独有, 输入为 s, 输出为 $\mathcal{A}^i$ 的概率分布</p>
<p>$\nabla<em>{\theta^i} J(\theta^1,…,\theta^m) = \mathbb{E}</em>{S,A}[(Q<em>\pi(S,A) - V</em>\pi(S))\cdot\nabla_{\theta^i} \ln\pi(A^i\mid S;\theta^i)]$</p>
<p>$g^i(s,a;\theta^i) = (Q<em>\pi(s,a) - V</em>\pi(s))\cdot\nabla_{\theta^i} \ln\pi(a^i\mid s;\theta^i)$</p>
<p>$g^i(s<em>t,a_t;\theta^i) = (r_t+\gamma \cdot v(s</em>{t+1};w) - v<em>\pi(s_t;w))\cdot\nabla</em>{\theta^i} \ln\pi(a_t^i\mid s_t;\theta^i)$</p>
</li>
</ul>
<p>同策略 $\pi(\cdot\mid s_t;\theta^i)$</p>
<p>Agent 不能独立做决策, 需要知道所有 Agent 的观测, 即全局状态.</p>
<p><img src="1.png" alt=""></p>
<p>trade-off: 通信慢, 但不近似网络, 效果好.</p>
<ul>
<li>中心化训练+中心化决策: 1 个价值网络 + m 个策略网络, 中央收集各 Agent 的观测, 分配决策, Agent 完全不用思考. 由于必须等待所有观测组成状态, 产生短板效应, 延迟高.</li>
<li>去中心化训练+去中心化决策: m 个价值网络(输入为 $o^i$ 而非 $s$) + m 个策略网络(输入为 $o^i$ 而非 $s$), Agent 之间不通信, 参数独立, 本质为 SARL.</li>
<li>中心化训练+去中心化决策: 1 个价值网络 + m 个策略网络(输入为 $o^i$ 而非 $s$), 中央收集各 Agent 的观测, 训练价值网络的同时将 TD 误差广播到所有 Agent ,每个 Agent 更新自己的策略网络.</li>
</ul>
<h1 id="十五-非合作关系-MARL"><a href="#十五-非合作关系-MARL" class="headerlink" title="十五 非合作关系 MARL"></a>十五 非合作关系 MARL</h1><p>$S = [O^1,O^2,…,O^m]$</p>
<p>$R,U,Q,V,J$​​ 不相同</p>
<p>$J^i(\theta^1,…,\theta^m) = \mathbb{E}[V_\pi^i(s)]$​</p>
<p>每个 Agent 求解 $\max_{\theta^i} J^i(\theta^1,…,\theta^m)$</p>
<p>收敛判别为纳什均衡: 当其余所有 Agent 都不改变策略的情况下, 一个 Agent $i$ 单独改变策略 $\theta^i$​ 无法令 $J^i$ 变大.</p>
<p>两次学习优劣评价: 交换对手.</p>
<h3 id="MAN-A2C"><a href="#MAN-A2C" class="headerlink" title="MAN-A2C"></a>MAN-A2C</h3><p>Multi-Agent Non-cooperative A2C</p>
<p>m 个策略网络 + m 个价值网络</p>
<p>$\nabla<em>{\theta^i} J(\theta^1,…,\theta^m) = \mathbb{E}</em>{S,A}[(Q<em>\pi^i(S,A) - V</em>\pi^i(S))\cdot\nabla_{\theta^i} \ln\pi(A^i\mid S;\theta^i)]$​</p>
<p>  $g^i(s<em>t,a_t^i;\theta^i) = (r_t^i+\gamma \cdot v(s</em>{t+1};w^i) - v<em>\pi(s_t;w^i))\cdot\nabla</em>{\theta^i} \ln\pi(a_t^i\mid s_t;\theta^i)$​</p>
<p>同策略 $\pi(\cdot\mid s_t;\theta^i)$​</p>
<ul>
<li>中心化训练+中心化决策: 中央收集各 Agent 的观测, m 个评委分别评价各个Agent, m 个策略网络产生动作.</li>
<li>去中心化训练+去中心化决策: 价值网络与策略网络输入为 $o^i$ 而非 $s$, Agent 之间不通信, 参数独立, 奖赏各不相同. 本质为 SARL.</li>
<li>中心化训练+去中心化决策: 策略网络输入为 $o^i$ 而非 $s$​​​​, 与 MAC-A2C 的中心化训练+去中心化决策基本相同, 区别在于有 m 个价值网络而非 1 个.</li>
</ul>
<hr>
<h3 id="MADDPG"><a href="#MADDPG" class="headerlink" title="MADDPG"></a>MADDPG</h3><p>Multi-Agent Deep Deterministic Policy Gradient 多智能体深度确定策略梯度</p>
<p>连续控制 异策略 中心化训练+去中心化决策</p>
<p>训练单个 Agent 的策略网络与价值网络需要一个经验回放与所有策略网络, 因此只能中心化训练.</p>
<h1 id="十六-注意力机制与-MARL"><a href="#十六-注意力机制与-MARL" class="headerlink" title="十六 注意力机制与 MARL"></a>十六 注意力机制与 MARL</h1><p>自注意力层 Self-Attention Layer: 输入为 $(x^1,…,x^m)$, 输出为 $(c^1,…,c^m)$, $m$ 可变, $c^i$ 与 $(x^1,…,x^m)$ 相关. 此前的 RNN 只与  $(x^1,…,x^i)$​ 相关.</p>
<p>$ q^i = W_q x^i ,  ~k^i = W_k x^i , ~v^i = W_v x^i$</p>
<p>$\alpha^i = softmax[(q^i)^\top k^1,…,(q^i)^\top k^m]$</p>
<p>$c^i = [v^1,…,v^m]\cdot \alpha^i = \alpha_1^i v^1 + … + \alpha_m^i v^m$</p>
<p>$c^i$ 主要取决于最密切相关的一个或几个 $x~\rightarrow~ $一个 Agent 学会判断哪些 Agent 最相关并重点关注. $c^i$ 作为全连接网络的输入. </p>
<p>多头(Multi-Head)自注意力层将单头(Single-Head)自注意力层的输出连接起来.</p>
<p>可用于策略网络与价值网络, 离散与连续.</p>
<p>可将中心化的 $m$ 个价值网络/策略网络用 1 个带自注意力层的网络实现. $m$​ 越大效果越好.</p>
<h1 id="十七-模仿学习"><a href="#十七-模仿学习" class="headerlink" title="十七 模仿学习"></a>十七 模仿学习</h1><p>Imitation Learning</p>
<p>不是强化学习. 通过向人类专家学习策略网络, 使其做出与专家相同决策, 而非强化学习使累计奖励最大化. 训练成本低 (无人驾驶不用撞车).</p>
<h3 id="行为克隆"><a href="#行为克隆" class="headerlink" title="行为克隆"></a>行为克隆</h3><p>本质为监督学习, 数据集为 {(状态, 动作)}, 不需要与环境交互.</p>
<ul>
<li>连续控制: 回归</li>
<li>离散控制: 多类别分类器, 输出各动作概率</li>
</ul>
<h3 id="逆向强化学习-Inverse-RL"><a href="#逆向强化学习-Inverse-RL" class="headerlink" title="逆向强化学习 Inverse RL"></a>逆向强化学习 Inverse RL</h3><p>根据人类专家的策略学习奖赏关于状态与动作的函数, 再用其训练策略.</p>
<h3 id="GAIL"><a href="#GAIL" class="headerlink" title="GAIL"></a>GAIL</h3><p>Generative Adversarial Imitation Learning</p>
<h4 id="生成判别网络-GAN"><a href="#生成判别网络-GAN" class="headerlink" title="生成判别网络 GAN"></a>生成判别网络 GAN</h4><p>Generative Adversarial Network</p>
<ul>
<li>生成器: $a = G(s;\theta)$, 由随机向量 $s$ 生成假样本</li>
<li>判别器: $\hat{p} = D(x;\Phi)$, 判断样本 $x$ 是真的概率</li>
</ul>
<p>同时训练两者, 增强生成器以假乱真与判别器判别的能力.</p>
<p>GAIL 的生成器是策略网络, 输入状态输出动作. 判别器输入状态, 输出 $|\mathcal{A}|$ 维向量 $\hat{p}$, 每个元素 $\hat{p}_a \in (0,1)$ 表示 $(s,a)$ 真假程度 ($a$ 是否为人类专家(真样本)所做)</p>
<h1 id="十八-AlphaGo-与-MCTS"><a href="#十八-AlphaGo-与-MCTS" class="headerlink" title="十八 AlphaGo 与 MCTS"></a>十八 AlphaGo 与 MCTS</h1>]]></content>
  </entry>
  <entry>
    <title>DSP note</title>
    <url>/2021/09/13/DSP_note/</url>
    <content><![CDATA[<p>Digital Systems Processing</p>
<h1 id="Lecture-1"><a href="#Lecture-1" class="headerlink" title="Lecture 1"></a>Lecture 1</h1><p>信号是信息的载体, 信号 ~ 函数</p>
<ul>
<li><p>时间为自变量</p>
<ul>
<li><p>连续信号 $x(t)$​ -&gt; 模拟信号: 取值也连续​</p>
</li>
<li><p>离散信号 $x[n]$​ -&gt; 数字信号: 取值也离散</p>
</li>
</ul>
</li>
</ul>
<p>模拟信号 </p>
<p>—&gt;(信号采样) 抽样信号, $t$​ 由连续变为离散 </p>
<p>—&gt;(数值量化)  数字信号, 取值由连续变为离散, 只有一些固定点上有取值 (000, 001, 010, …)</p>
<p>—&gt;(零阶保持)  量化信号, $t$​​​ 由离散恢复为连续, 取值依然离散 (把点用线段连起来?)</p>
<p>—&gt;(低通滤波)  模拟信号, 取值恢复为连续</p>
<p>确定信号: 信号随时间变化服从确定规律, 能够以确定时间函数表示.</p>
<p>系统的输入(激励), 输出(响应)都是信号(函数), 对信号进行加工, 变换.</p>
<p>FFT: Fast Fourier Transformation</p>
<p>时域: 以时间为自变量描述信号和系统</p>
<p>空域: 以空间为自变量描述信号和系统 (图片)</p>
<p>频域: 以频率为自变量描述信号和系统. 如果不以频率为基本单位, 也成为变换域.</p>
<p>频谱: 信号的频域表示, 描述信号各个频率分量的幅度和相位.</p>
<p>滤波器: 一种信号处理系统, 实现按照一定目的对信号的变换/过滤 (过滤频率)</p>
<p>时域 ~~ 傅里叶变换 ~~ 频域 (从频率角度提取特征)</p>
]]></content>
  </entry>
  <entry>
    <title>HSEA note</title>
    <url>/2021/09/13/HSEA_note/</url>
    <content><![CDATA[<p>Heuristic Search and Evolutionary Algorithms</p>
<h1 id="Lecture-1"><a href="#Lecture-1" class="headerlink" title="Lecture 1"></a>Lecture 1</h1><p>A search problem</p>
<ul>
<li>initial state (what about state space?)</li>
<li>actions</li>
<li>transition model</li>
<li>foal test</li>
<li>path cost</li>
</ul>
<p>Solution: a path</p>
<hr>
<p><strong>complexity</strong></p>
]]></content>
  </entry>
</search>
